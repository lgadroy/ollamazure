# ollamazure

## Installation

Une fois la VM Azure cr√©√©e (je vous passe les d√©tails car la cr√©ation de VM est d√©j√† guid√©e en interface), nous pouvons d√©marrer l'installation de nos outils. On se connecte √† la machine (via Putty ou Azure CLI), et on d√©marre l'installation d'Ollama. J'inclus √©galement ci-dessous l'installation de l'outil htop, qui sert √† voir l'utilisation des ressources de votre machine. Vous n'√™tes pas oblig√© de l'installer si vous n'en voulez pas.
```bash
sudo apt update && sudo apt upgrade
curl -fsSL https://ollama.com/install.sh | sh
sudo apt install htop
```

On t√©l√©charge maintenant le mod√®le de notre choix, les noms de mod√®les pouvant √™tre install√©s sont trouvables directement sur ollama.com. J'installe de mon c√¥t√© une version l√©g√®re de llama3.2.
```bash
ollama pull llama3.2:1b
```

Il nous reste une derni√®re chose √† faire sur Ollama, faire en sorte qu'il √©coute sur toutes les interfaces. Si vous ne faites pas cela Ollama ne pourra pas communiquer avec votre interface web tout-√†-l'heure et vous n'aurez pas acc√®s aux mod√®les que vous avez install√©.
```bash
nano /etc/systemd/system/ollama/service
```

```bash
# Dans le bloc ci-dessous ajoutez cette variable d'environnement :
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
```

Il reste ensuite √† red√©marrer Ollama.
```bash
systemctl daemon-reload
systemctl restart ollama.service
```

‚úÖ Ollama est install√© et pr√™t √† √™tre utilis√©. 

Passons √† l'installation d'un outil de conteneurisation. Il est possible d'installer Open WebUI via Python, et Nginx via la commande apt, mais la conteneurisation pr√©sente plusieurs avantages. Elle nous permet de garantir une isolation des applications, r√©duisant alors le risque de conflits entre les d√©pendances, et elle facilite les mises √† jour des diff√©rents services.

Je choisis d'installer Podman, une alternative √† Docker d√©velopp√©e par RedHat qu'il est possible d'installer en une seule commande. Je vais ensuite aller chercher les images d'Open WebUI et de Nginx.
```bash
sudo apt install -y podman
podman network create mynet
podman pull ghcr.io/open-webui/open-webui:main
podman pull docker.io/nginx
```

Pour que notre conteneur Open WebUI puisse communiquer avec notre h√¥te et donc interagir avec Ollama, nous devons obtenir l'adresse IP interne de notre h√¥te avec cette commande :
```bash
ip -4 addr show dev eth0
```

Nous pouvons ensuite d√©ployer notre conteneur Open WebUI (changez l'IP via l'IP interne de votre h√¥te).
```bash
podman run -d --network mynet -p 3000:8080 --add-host=host.containers.internal:10.0.0.4 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

‚úÖ L'interface web Open WebUI est d√©ploy√©e.

Cependant elle n'est pas encore accessible, vous pouvez y acc√©der tout de suite en ouvrant le port 3000 de votre machine, mais je ne vous le recommande pas, vos communications n'√©tant pas encore chiffr√©es gr√¢ce au protocole HTTPS.

Pour le certificat j'utiliserais un certificat auto-sign√© mais vous pouvez utiliser un certificat Let's Encrypt si vous le souhaitez. Je commence donc par cr√©er le dossier o√π seront stock√©es les cl√©s publiques et priv√©es. Dans la 2e commande n'oubliez pas de remplacer le CN (Common Name) par votre IP ou votre nom de domaine :
```bash
mkdir -p ~/nginx/cert

openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout ~/nginx/cert/selfsigned.key -out ~/nginx/cert/selfsigned.crt -subj "/C=FR/ST=France/L=Paris/O=Company/CN=IP-ou-nomdedomaine"
```

Nous allons ensuite cr√©er la configuration de notre reverse-proxy Nginx. Je limite le nombre de connexions simultan√©es √† 1024. Vous remarquerez aussi l'ajout du support des WebSocket. Sans cela, lorsque l'on tente une requ√™te vers un mod√®le d'IA on obtient une erreur car la g√©n√©ration du message de l'IA se fait en temps r√©el et n√©cessite ce type de connexion.
```bash
nano ~/nginx/nginx.conf
```

```conf
user nginx;
worker_processes auto;

events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    server {
        listen 80;
        listen 443 ssl;
        server_name IP-ou-nomdedomaine;  # Remplacez par votre IP/Nom de domaine

        ssl_certificate /etc/nginx/cert/selfsigned.crt;
        ssl_certificate_key /etc/nginx/cert/selfsigned.key;

        location / {
            proxy_pass http://open-webui:8080;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Support WebSocket
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
        }
    }
}
```

On peut maintenant d√©ployer notre conteneur Nginx, en prenant en compte l'emplacement de nos fichiers de configuration.
```bash
podman run -d --network mynet --name nginx -p 443:443 -v ~/nginx/nginx.conf:/etc/nginx/nginx.conf:ro -v ~/nginx/cert:/etc/nginx/cert:ro nginx
```

‚úÖ Nginx est d√©ploy√© et l'interface web Open WebUI est accessible.

## Acc√®s √† l'interface

Une fois ceci fait vous devriez pouvoir acc√©der √† l'interface via le HTTPS. Si ce n'est pas le cas v√©rifiez que vous avez ouvert le port via le pare-feu Azure, vous devez avoir une r√®gle qui ressemble √† la 2e de l'image ci-dessous, sinon cr√©ez-l√†.

![Alt text](images/azure.png?raw=true "azure")

Sur l'interface on vous demande de cr√©er votre premier utilisateur qui sera aussi Administrateur, et de lui attribuer un mot de passe, une fois ceci fait vous acc√©dez √† l'interface.

![Alt text](images/interface.png?raw=true "interfaces")

Vous n'acc√©derez pas tout de suite aux mod√®les car il reste une chose √† changer. Par d√©faut Open WebUI utilise Docker mais nous utilisons Podman et cela pose un petit soucis dans les param√®tres. Cliquez sur le logo de votre utilisateur -> Param√®tres -> Param√®tres admin. -> Connexions. Vous devez changer "http://host.docker.internal:11434" par ceci :

![Alt text](images/connection.png?raw=true "connection")

‚úÖ Vous pouvez d√©sormais acc√©der √† vos mod√®les et commencer √† discuter.

![Alt text](images/message.png?raw=true "presentation")

> N'oubliez pas d'explorer les options d'utilisateurs et de groupes d'utilisateurs afin de cr√©er des utilisateurs pouvant uniquement communiquer avec les mod√®les que vous leur proposez. En session administrateur vous pouvez cr√©er des mod√®les pour les utilisateurs, des prompts, ajouter une base de connaissances, et plus encore, ce qui fait beaucoup plus d'autorisations que n√©cessaire. Pensez √† vous cr√©er un utilisateur avec des droits appropri√©s.

## Conclusion

Vous savez maintenant comment installer Ollama sur une machine virtuelle d√©ploy√©e dans le Cloud, et communiquer avec les mod√®les propos√©s √† travers une interface web open-source, le tout s√©curis√© via le protocole HTTPS. Bravo! üéâ